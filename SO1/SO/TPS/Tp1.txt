TP1 PRACTICA:

1)Se denomina multiprogramación a la técnica que permite que dos o más procesos ocupen la misma unidad de memoria principal. Multiprogramacion implica multiproceso,sin embargo multiproceso no implica multiprogramación. Aporta las siguientes ventajas:

Aumenta el uso de la CPU.
Las direcciones de los procesos son relativas, el programador no se preocupa por saber en dónde estará el proceso dado que el sistema operativo es el que se encarga de convertir la dirección lógica en física.

Ejecución de múltiples tareas compartiendo los recursos de una misma computadora. Se trata de una evolución del procesamiento por lotes. Dentro de la multiprogramación se encuentra el concepto de tiempo compartido, en donde cada usuario tiene acceso a una única computadora a través de terminales. La computadora central se simula ejecutar en simultáneo las tareas de cada usuario.

multiprogramación: Un sistema operativo que puede ejecutar más de un programa a la vez. compartiendo el tiempo y la memoria de una sola CPU (central programming unit) La multiprogramación es una forma rudimentaria de procesamiento paralelo, donde varios programas son ejecutados al mismo tiempo sobre un único procesador. Como hay un solo procesador, no puede haber una verdadera ejecución simultánea de programas diferentes. El sistema operativo ejecuta parte de un programa, luego parte de otro, y así al usuario le parece que todos los procesos son ejecutados al mismo tiempo...

2)
El acceso directo a memoria (DMA, del inglés Direct Memory Access) permite a cierto tipo de componentes de ordenador acceder a la memoria del sistema para leer o escribir independientemente de la CPU principal. Muchos sistemas hardware utilizan DMA, incluyendo controladores de unidades de disco, tarjetas gráficas y tarjetas de sonido. DMA es una característica esencial en todos los ordenadores modernos, ya que permite a dispositivos de diferentes velocidades comunicarse sin someter a la CPU a una carga masiva de interrupciones.
Una transferencia DMA consiste principalmente en copiar un bloque de memoria de un dispositivo a otro. En lugar de que la CPU inicie la transferencia, la transferencia se lleva a cabo por el controlador DMA. Un ejemplo típico es mover un bloque de memoria desde una memoria externa a una interna más rápida. Tal operación no ocupa al procesador y como resultado éste puede ser planificado para efectuar otras tareas. Las transferencias DMA son esenciales para aumentar el rendimiento de aplicaciones que requieran muchos recursos.
Cabe destacar que aunque no se necesite a la CPU para la transacción de datos, sí que se necesita el bus del sistema (tanto bus de datos como bus de direcciones), por lo que existen diferentes estrategias para regular su uso, permitiendo así que no quede totalmente acaparado por el controlador DMA.

En lugar de que la CPU inicie la transferencia, la transferencia se lleva a cabo por el controlador DMA. Un ejemplo típico es mover un bloque de memoria desde una memoria externa a una interna más rápida. Tal operación no ocupa al procesador y como resultado éste puede ser planificado para efectuar otras tareas. Implica que el CPU este ocupado para realizar transferencias de memoria por ejemplo, lo que implica que la cpu tuviera mas uso y la multiprogramacion se vea afectada.

3)La unidad de gestión de memoria (Esp.) o unidad de manejo de memoria (Lat.) (en inglés: Memory Management Unit o simplemente MMU) es un dispositivo de Hardware formado por un grupo de circuitos integrados, responsable del manejo de los accesos a la memoria por parte de la Unidad de Procesamiento Central (CPU).
Entre las funciones de este dispositivo se encuentran la traducción de las direcciones lógicas (o virtuales) a direcciones físicas (o reales), la protección de la memoria, el control de caché y, en arquitecturas de computadoras más simples (especialmente en sistemas de 8 bits), Bank switching.
Cuando la CPU intenta acceder a una dirección de memoria lógica, la MMU realiza una búsqueda en una memoria caché especial llamada Buffer de Traducción Adelantada (TLB, Translation Lookaside Buffer), que mantiene la parte de la tabla de páginas usada hace menos tiempo. En esta memoria se mantienen entradas de la tabla de páginas (llamadas PTE por sus siglas en inglés, Page Table Entry), donde se pueden rescatar las direcciones físicas correspondientes a algunas direcciones lógicas, de forma directa. Cuando la dirección requerida por la CPU se encuentra en el TLB, su traducción a dirección real o física es entregada, en lo que se conoce como 'acierto en el TLB' ('TLB hit'). En otro caso, cuando la dirección buscada no se encuentra en el TLB (fallo en el TLB), el procesador busca en la tabla de páginas del proceso utilizando el número de página como entrada a la misma. En la entrada de la tabla de páginas del proceso se encuentra un bit de presencia, que indica si la página buscada está en memoria principal. Si el bit de presencia está activado, se carga esta PTE en el TLB y se devuelve la dirección física. En caso contrario, se informa al sistema operativo de la situación, mediante un fallo de página. Es el sistema operativo el encargado de realizar los ajustes necesarios (esto es, cargar la página en memoria física) usando uno de los Algoritmos de reemplazo de páginas, para continuar con la ejecución desde la instrucción que causó el fallo.
Un beneficio fundamental de la MMU es la posibilidad de implementar protección de memoria, evitando que los programas accedan a porciones de memoria prohibidas. Por ejemplo se puede evitar que un programa acceda o modifique sectores de memoria de otros programas.

4)

5)






7)Fork-exec is a commonly used technique in Unix whereby an executing process spawns a new program. fork() is the name of the system call that the parent process uses to "divide" itself ("fork" into two identical processes). After calling fork(), the created child process is actually an exact copy of the parent - which would probably be of limited use - so it replaces itself with another process using the system call exec().
When a process forks, a complete copy of the executing program is made into the new process. This new process (which is a child of the parent) has a new process identifier (PID). The fork() function returns the child's PID to the parent, while it returns 0 to the child, in order to allow the two identical processes to distinguish one another.
The parent process can either continue execution or wait for the child process to complete. The child, after discovering that it is the child, replaces itself completely with another program, so that the code and address space of the original program are lost.
If the parent chooses to wait for the child to die, then the parent will receive the exit code of the program that the child executed. Otherwise, the parent can ignore the child process and continue executing as it normally would; to prevent the child becoming a zombie it should wait on children at intervals or on SIGCHLD.
When the child process calls exec(), all data in the original program is lost, and replaced with a running copy of the new program. This is known as overlaying. Although all data is replaced, the file descriptors that were open in the parent are closed only if the program has explicitly marked them close-on-exec. This allows for the common practice of the parent creating a pipe prior to calling fork() and using it to communicate with the executed program.
Windows does not support the fork-exec technique as it does not allow to fork a process. The spawn() family of functions declared in process.h can replace it in cases where the call to fork() is followed directly by exec().